{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxcP_mGtAvt6"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYjhb_KHBR03",
        "outputId": "c74368bd-e61b-4c3f-fe25-70ffd3db0755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2APxThjBTCQ"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAwLz_KPDmdq"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oFjctYSFEySn"
      },
      "outputs": [],
      "source": [
        "# Create directories to store the training, testing and validation data respectively\n",
        "!mkdir train\n",
        "!mkdir test\n",
        "!mkdir valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdfhPn7kDmXJ",
        "outputId": "9d6cee71-8ec2-4b49-aca1-e31b9e26a6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading real-life-violence-situations-dataset.zip to /content\n",
            "100% 3.57G/3.58G [00:27<00:00, 157MB/s]\n",
            "100% 3.58G/3.58G [00:27<00:00, 140MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the RLVS dataset\n",
        "!kaggle datasets download mohamedmustafa/real-life-violence-situations-dataset -p /content/ --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m090uKZ2DmT_"
      },
      "outputs": [],
      "source": [
        "# Get the video names of violence and non-violence\n",
        "import os\n",
        "non_violent=os.listdir(\"/content/Real Life Violence Dataset/NonViolence\")\n",
        "violent=os.listdir(\"/content/Real Life Violence Dataset/Violence\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLZROgpLDmNX"
      },
      "outputs": [],
      "source": [
        "# Splitting the entire dataset into 8:1:1 for train:validation:test\n",
        "import random\n",
        "def split_test_train(lis):\n",
        "  n=len(lis)\n",
        "  other=random.sample(range(n),int(n/10))\n",
        "  return ([x for i,x in enumerate(lis) if not i in other],[x for i,x in enumerate(lis) if i in other])\n",
        "\n",
        "def split_train_valid(lis):\n",
        "  n=len(lis)\n",
        "  other=random.sample(range(n),int(n/9))\n",
        "  return ([x for i,x in enumerate(lis) if not i in other],[x for i,x in enumerate(lis) if i in other])\n",
        "\n",
        "train_non_violent,test_non_violent=split_test_train(non_violent)\n",
        "train_violent,test_violent=split_test_train(violent)\n",
        "\n",
        "train_non_violent,valid_non_violent=split_train_valid(train_non_violent)\n",
        "train_violent,valid_violent=split_train_valid(train_violent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ6GDweuDl-g"
      },
      "outputs": [],
      "source": [
        "# Moving the videos into the appropriate folder based on the split\n",
        "import shutil\n",
        "src_path=\"/content/Real Life Violence Dataset/NonViolence/\"\n",
        "for filename in train_non_violent:\n",
        "  shutil.move(os.path.join(src_path,filename), \"/content/train\")\n",
        "for filename in test_non_violent:\n",
        "  shutil.move(os.path.join(src_path,filename), \"/content/test\")\n",
        "for filename in valid_non_violent:\n",
        "  shutil.move(os.path.join(src_path,filename), \"/content/valid\")\n",
        "\n",
        "\n",
        "src_path=\"/content/Real Life Violence Dataset/Violence/\"\n",
        "for filename in train_violent:\n",
        "  shutil.move(os.path.join(src_path,filename), \"/content/train\")\n",
        "for filename in test_violent:\n",
        "  shutil.move(os.path.join(src_path,filename), \"/content/test\")\n",
        "for filename in valid_violent:\n",
        "  shutil.move(os.path.join(src_path,filename), \"/content/valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJexFfG7WDmb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train=[]\n",
        "test=[]\n",
        "valid=[]\n",
        "for filename in train_non_violent:\n",
        "  train.append([filename,\"Non Violent\"])\n",
        "for filename in train_violent:\n",
        "  train.append([filename,\"Violent\"])\n",
        "for filename in test_non_violent:\n",
        "  test.append([filename,\"Non Violent\"])\n",
        "for filename in test_violent:\n",
        "  test.append([filename,\"Violent\"])\n",
        "for filename in valid_non_violent:\n",
        "  valid.append([filename,\"Non Violent\"])\n",
        "for filename in valid_violent:\n",
        "  valid.append([filename,\"Violent\"])\n",
        "\n",
        "# Store a dataframe storing the video name and its label(violent or non-violent)\n",
        "train_df = pd.DataFrame(train, columns =[\"video_name\",\"tag\"])\n",
        "test_df = pd.DataFrame(test, columns =[\"video_name\",\"tag\"])\n",
        "valid_df=pd.DataFrame(valid, columns =[\"video_name\",\"tag\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KACEqXbYlUD"
      },
      "outputs": [],
      "source": [
        "# Randomly shuffle them\n",
        "train_df=train_df.sample(frac = 1)\n",
        "test_df=test_df.sample(frac = 1)\n",
        "valid_df=valid_df.sample(frac = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HeM-Z98UDRvH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "import io\n",
        "import ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoA2IConERpL"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 30 # The maximum number of frames extracted from each video\n",
        "IMG_SIZE = 90 # the width/lenght of each frame extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V00H-wy0ihaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725ca261-913f-40ec-e28d-62f1ab6473e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1600\n",
            "Total videos for testing: 200\n",
            "Total videos for validating: 200\n",
            "['Non Violent', 'Violent']\n"
          ]
        }
      ],
      "source": [
        "# Layer for cropping the frame to the desired size\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE,IMG_SIZE)\n",
        "\n",
        "def crop_center(frame):\n",
        "    '''Crops the frame and outputs a numpy array'''\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "def load_video(path, max_frames):\n",
        "    '''Extracts frames at fixed intervals throught the video till the video ends or the max_seq_lenght is reached'''\n",
        "    frames_to_take_per_sec=5\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frames = []\n",
        "    \n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    k=int(fps/frames_to_take_per_sec)\n",
        "    i=0\n",
        "    j=0\n",
        "    try:\n",
        "      while j<max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "          break\n",
        "        if i%k==0:\n",
        "          j+=1\n",
        "          frame= crop_center(frame)\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "          frames.append(frame)\n",
        "        i+=1\n",
        "    finally:\n",
        "      cap.release()\n",
        "      return np.array(frames[:max_frames])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generates the label for the set of frames\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    '''Converts all the videos in the folder to the desired max_seq_lenght and img_size \n",
        "    along with their labels for further processing'''\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels).numpy()\n",
        "\n",
        "    videos = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, IMG_SIZE,IMG_SIZE), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path),MAX_SEQ_LENGTH)\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.empty((diff, IMG_SIZE, IMG_SIZE), dtype=\"float32\")\n",
        "            for d in range(diff):\n",
        "              padding[d:]=frames[-1:]\n",
        "            frames = np.concatenate((frames, padding))\n",
        "\n",
        "        videos[idx,] = frames\n",
        "\n",
        "    return videos, labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQw9ZcZgi2PB"
      },
      "outputs": [],
      "source": [
        "train_videos, train_labels = prepare_all_videos(train_df,\"/content/train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91SWIXvcn-pk"
      },
      "outputs": [],
      "source": [
        "test_videos, test_labels = prepare_all_videos(test_df,\"/content/test\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg81O8zIn-aN"
      },
      "outputs": [],
      "source": [
        "valid_videos, valid_labels = prepare_all_videos(valid_df,\"/content/valid\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-x4FZg89ud_"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 1)\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 40\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY784Hh59ueC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAlf8PPh9ueF"
      },
      "outputs": [],
      "source": [
        "\n",
        "@tf.function\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
        "    # Preprocess images\n",
        "    frames = tf.image.convert_image_dtype(\n",
        "        frames[\n",
        "            ..., tf.newaxis\n",
        "        ],  # The new axis is to help for further processing with Conv3D layers\n",
        "        tf.float32,\n",
        "    )\n",
        "    # Parse label\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "\n",
        "    if loader_type == \"train\":\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "\n",
        "    dataloader = (\n",
        "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
        "train_videos=[] \n",
        "train_labels=[]\n",
        "\n",
        "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
        "valid_videos=[] \n",
        "valid_labels=[]\n",
        "\n",
        "testloader = prepare_dataloader(test_videos, test_labels, \"test\")\n",
        "test_videos=[] \n",
        "test_labels=[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2VeBuE09ueI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZeua8il9ueK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEbCWo0i9ueM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.relu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.relu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"sigmoid\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c88mD7rk9ueN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3070cc5-328a-486e-9c2b-4c6357ed19b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "50/50 [==============================] - 30s 303ms/step - loss: 0.8171 - accuracy: 0.4888 - val_loss: 0.6852 - val_accuracy: 0.5400\n",
            "Epoch 2/40\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.7081 - accuracy: 0.4869 - val_loss: 0.6863 - val_accuracy: 0.5050\n",
            "Epoch 3/40\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.7021 - accuracy: 0.4825 - val_loss: 0.6967 - val_accuracy: 0.5000\n",
            "Epoch 4/40\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.7016 - accuracy: 0.5169 - val_loss: 0.6776 - val_accuracy: 0.5150\n",
            "Epoch 5/40\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 0.6908 - accuracy: 0.5113 - val_loss: 0.6756 - val_accuracy: 0.5450\n",
            "Epoch 6/40\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.6841 - accuracy: 0.5419 - val_loss: 0.6750 - val_accuracy: 0.6100\n",
            "Epoch 7/40\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.6928 - accuracy: 0.5125 - val_loss: 0.6870 - val_accuracy: 0.5250\n",
            "Epoch 8/40\n",
            "50/50 [==============================] - 14s 286ms/step - loss: 0.6847 - accuracy: 0.5300 - val_loss: 0.6849 - val_accuracy: 0.5300\n",
            "Epoch 9/40\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.6789 - accuracy: 0.5506 - val_loss: 0.6748 - val_accuracy: 0.5750\n",
            "Epoch 10/40\n",
            "50/50 [==============================] - 14s 286ms/step - loss: 0.6722 - accuracy: 0.5600 - val_loss: 0.6737 - val_accuracy: 0.5650\n",
            "Epoch 11/40\n",
            "50/50 [==============================] - 14s 286ms/step - loss: 0.6493 - accuracy: 0.6019 - val_loss: 0.6476 - val_accuracy: 0.6000\n",
            "Epoch 12/40\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.6078 - accuracy: 0.6619 - val_loss: 0.5797 - val_accuracy: 0.6800\n",
            "Epoch 13/40\n",
            "50/50 [==============================] - 14s 287ms/step - loss: 0.5708 - accuracy: 0.6938 - val_loss: 0.5870 - val_accuracy: 0.6650\n",
            "Epoch 14/40\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.4553 - accuracy: 0.7812 - val_loss: 0.4250 - val_accuracy: 0.8000\n",
            "Epoch 15/40\n",
            "50/50 [==============================] - 15s 289ms/step - loss: 0.4322 - accuracy: 0.8087 - val_loss: 0.4186 - val_accuracy: 0.8050\n",
            "Epoch 16/40\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.3858 - accuracy: 0.8319 - val_loss: 0.3704 - val_accuracy: 0.8450\n",
            "Epoch 17/40\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.3696 - accuracy: 0.8413 - val_loss: 0.3996 - val_accuracy: 0.8200\n",
            "Epoch 18/40\n",
            "50/50 [==============================] - 14s 289ms/step - loss: 0.3589 - accuracy: 0.8400 - val_loss: 0.3678 - val_accuracy: 0.8500\n",
            "Epoch 19/40\n",
            "50/50 [==============================] - 14s 289ms/step - loss: 0.3583 - accuracy: 0.8462 - val_loss: 0.3730 - val_accuracy: 0.8500\n",
            "Epoch 20/40\n",
            "50/50 [==============================] - 14s 289ms/step - loss: 0.3343 - accuracy: 0.8531 - val_loss: 0.3851 - val_accuracy: 0.8250\n",
            "Epoch 21/40\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.3385 - accuracy: 0.8506 - val_loss: 0.4334 - val_accuracy: 0.7650\n",
            "Epoch 22/40\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.3177 - accuracy: 0.8644 - val_loss: 0.3800 - val_accuracy: 0.8400\n",
            "Epoch 23/40\n",
            "50/50 [==============================] - 15s 291ms/step - loss: 0.2977 - accuracy: 0.8763 - val_loss: 0.3904 - val_accuracy: 0.8400\n",
            "Epoch 24/40\n",
            "50/50 [==============================] - 14s 290ms/step - loss: 0.2951 - accuracy: 0.8681 - val_loss: 0.4236 - val_accuracy: 0.8250\n",
            "Epoch 25/40\n",
            "50/50 [==============================] - 14s 289ms/step - loss: 0.2934 - accuracy: 0.8706 - val_loss: 0.4084 - val_accuracy: 0.8250\n",
            "Epoch 26/40\n",
            "50/50 [==============================] - 15s 290ms/step - loss: 0.2811 - accuracy: 0.8781 - val_loss: 0.4369 - val_accuracy: 0.8250\n",
            "Epoch 27/40\n",
            "50/50 [==============================] - 14s 289ms/step - loss: 0.2569 - accuracy: 0.8956 - val_loss: 0.4488 - val_accuracy: 0.8200\n",
            "Epoch 28/40\n",
            "50/50 [==============================] - 15s 290ms/step - loss: 0.2421 - accuracy: 0.8925 - val_loss: 0.4571 - val_accuracy: 0.8050\n",
            "Epoch 29/40\n",
            "50/50 [==============================] - 15s 289ms/step - loss: 0.2401 - accuracy: 0.9000 - val_loss: 0.4410 - val_accuracy: 0.8200\n",
            "Epoch 30/40\n",
            "50/50 [==============================] - 15s 289ms/step - loss: 0.2218 - accuracy: 0.9125 - val_loss: 0.4996 - val_accuracy: 0.8100\n",
            "Epoch 31/40\n",
            "50/50 [==============================] - 15s 292ms/step - loss: 0.2015 - accuracy: 0.9206 - val_loss: 0.5088 - val_accuracy: 0.7950\n",
            "Epoch 32/40\n",
            "50/50 [==============================] - 15s 294ms/step - loss: 0.3256 - accuracy: 0.8550 - val_loss: 0.4628 - val_accuracy: 0.7800\n",
            "Epoch 33/40\n",
            "50/50 [==============================] - 15s 295ms/step - loss: 0.2433 - accuracy: 0.9062 - val_loss: 0.4578 - val_accuracy: 0.8150\n",
            "Epoch 34/40\n",
            "50/50 [==============================] - 15s 291ms/step - loss: 0.1917 - accuracy: 0.9262 - val_loss: 0.4881 - val_accuracy: 0.7950\n",
            "Epoch 35/40\n",
            "50/50 [==============================] - 15s 290ms/step - loss: 0.1675 - accuracy: 0.9406 - val_loss: 0.5503 - val_accuracy: 0.7900\n",
            "Epoch 36/40\n",
            "50/50 [==============================] - 15s 290ms/step - loss: 0.2138 - accuracy: 0.9144 - val_loss: 0.4080 - val_accuracy: 0.8200\n",
            "Epoch 37/40\n",
            "50/50 [==============================] - 15s 291ms/step - loss: 0.2124 - accuracy: 0.9175 - val_loss: 0.5055 - val_accuracy: 0.7950\n",
            "Epoch 38/40\n",
            "50/50 [==============================] - 15s 292ms/step - loss: 0.1699 - accuracy: 0.9362 - val_loss: 0.5207 - val_accuracy: 0.8000\n",
            "Epoch 39/40\n",
            "50/50 [==============================] - 15s 290ms/step - loss: 0.2360 - accuracy: 0.9087 - val_loss: 0.5326 - val_accuracy: 0.7750\n",
            "Epoch 40/40\n",
            "50/50 [==============================] - 15s 290ms/step - loss: 0.1415 - accuracy: 0.9538 - val_loss: 0.5818 - val_accuracy: 0.7850\n",
            "7/7 [==============================] - 1s 84ms/step - loss: 0.4106 - accuracy: 0.8700\n",
            "Test accuracy: 87.0%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def run_experiment():\n",
        "    # Initialize model\n",
        "    model = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "        ),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
        "    )\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
        "\n",
        "    _, accuracy = model.evaluate(testloader)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpaT7HsTHXLp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_single_video(frames):\n",
        "    '''Outputs the video in the desired img_size and seq_lenght'''\n",
        "    if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.empty((diff, IMG_SIZE, IMG_SIZE), dtype=\"float32\")\n",
        "            for d in range(diff):\n",
        "              padding[d:]=frames[-1:]\n",
        "            frames = np.concatenate((frames, padding))\n",
        "\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "def predict_action(path):\n",
        "    '''Uses the model to predict whether the video has violence or not'''\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "    frames = load_video(path,MAX_SEQ_LENGTH)\n",
        "    video = prepare_single_video(frames)\n",
        "    pred = model.predict(tf.expand_dims(video, axis=0))[0]\n",
        "    if pred>0.5:\n",
        "      return class_vocab[1]\n",
        "    else:\n",
        "      return class_vocab[0]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVUzWHA6GB9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2f83d5-0d69-413f-ad39-21141efc6b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 879ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Accuracy on videos with occlusions 95.12 %\n"
          ]
        }
      ],
      "source": [
        "# Checking the accuracy of the above model on occluded videos\n",
        "v=0\n",
        "for vid in os.listdir(\"/content/drive/MyDrive/Capstone Resources/data/vidsWithObstructions\"):\n",
        "  if predict_action(\"/content/drive/MyDrive/Capstone Resources/data/vidsWithObstructions/\"+vid)==\"Violent\":\n",
        "    v+=1\n",
        "print(\"Accuracy on videos with occlusions\",round(v*100/len(os.listdir(\"/content/drive/MyDrive/Capstone Resources/data/vidsWithObstructions\")),2),\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUtZMTshLIIm"
      },
      "outputs": [],
      "source": [
        "# Checking the accuracy of the above model on collected indoor violence videos\n",
        "v=0\n",
        "for vid in os.listdir(\"/content/drive/MyDrive/Capstone Resources/Phase 1/Videos/Clips/\"):\n",
        "  if predict_action(\"/content/drive/MyDrive/Capstone Resources/Phase 1/Videos/Clips/\"+vid)==\"Violent\":\n",
        "    v+=1\n",
        "print(\"Accuracy on videos with clips\",round(v*100/len(os.listdir(\"/content/drive/MyDrive/Capstone Resources/Phase 1/Videos/Clips/\")),2),\"%\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}